{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "direct-announcement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions1</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do you cook canned kidney beans</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how can i get the guy i like to like me again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can 46x30 mm rounds have more armor pierci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whats the difference in meaning between these ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is wikibuycom legit or a scam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          questions1  is_duplicate\n",
       "0                how do you cook canned kidney beans             0\n",
       "1      how can i get the guy i like to like me again             0\n",
       "2  how can 46x30 mm rounds have more armor pierci...             0\n",
       "3  whats the difference in meaning between these ...             0\n",
       "4                      is wikibuycom legit or a scam             0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Similarity-Modelling\n",
    "\n",
    "# XG 부스트 모델(데이터를 TF-IDF 과정을 거쳐서 학습)\n",
    "\n",
    "# 모델 구현\n",
    "import numpy as np\n",
    "np_load_old = np.load  # 전처리한 데이터 가져오기에서 np.load 실행시 오류가 있어서 수정\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "TRAIN_CLEAN_Q1_DATA = 'train_clean_q1.csv'\n",
    "TRAIN_CLEAN_Q2_DATA = 'train_clean_q2.csv'\n",
    "# TRAIN_LABEL_DATA_FILE = 'train_label.npy'\n",
    "\n",
    "#훈련 데이터 가져오기(문자열 데이터)\n",
    "train_q1_data = pd.read_csv( DATA_IN_PATH + TRAIN_CLEAN_Q1_DATA )\n",
    "train_q2_data = pd.read_csv( DATA_IN_PATH + TRAIN_CLEAN_Q2_DATA )\n",
    "# train_labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))\n",
    "train_q1_data.head()\n",
    "\n",
    "# train_labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-sweet",
   "metadata": {},
   "source": [
    "ValueError: Object arrays cannot be loaded when allow_pickle=False\n",
    " \n",
    "\n",
    "코드를 돌리던 중, 결과를 확인하려고 np.load를 이용했지만, 갑자기 위의 오류가 뜨면서 로드되지 않았다.\n",
    "\n",
    "검색을 통해 찾아보니, 해당 allow_pickle을 True로 바꿔주어 사용하면 오류를 해결할 수 있었다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#### 먼저 기존의 np.load를 np_load_old에 저장해둠.\n",
    "np_load_old = np.load\n",
    "\n",
    "#### 기존의 parameter을 바꿔줌\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "위의 코드를 이용하여 기존의 np.load를 바꿔주고, 다시 기존의 것을 사용하고 싶다면 다음과 같이 다시 불러오면 된다.\n",
    "\n",
    "np.load = np_load_old\n",
    "\n",
    "출처 : https://d-tail.tistory.com/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nuclear-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_input = list(train_q1_data['questions1'])\n",
    "train_q2_input = list( train_q2_data['questions2'])\n",
    "train_labels = list(train_q1_data['is_duplicate'])\n",
    "# print(train_q1_input[0:42])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supported-carrier",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data_df = pd.DataFrame({'questions1': train_q1_input, 'questions2': train_q2_input, 'is_duplicate': train_labels})\n",
    "# df = train_data_df\n",
    "# df_check = train_data_df.isnull()\n",
    "# print(df_check)\n",
    "\n",
    "# check_for_nan = df.isnull().values.any()\n",
    "# print(check_for_nan) #Nan값이 있다면 True출력\n",
    "\n",
    "# total_nan_values = df.isnull().sum().sum()\n",
    "# print(total_nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-gospel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_dop_row = df.dropna(axis=0)  # Nan 값이 있는 행 제거\n",
    "\n",
    "# total_nan_values = df_dop_row.isnull().sum().sum()\n",
    "# print(total_nan_values) #Nan 값이 몇개 있는지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thrown-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_q1_input = list(df_dop_row['questions1'])\n",
    "# train_q2_input = list(df_dop_row['questions2'])\n",
    "# train_labels = list(df_dop_row['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "established-brush",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-abf33b1abe05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# vectorizer = TfidfVectorizer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"char\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_q1_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_q1_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_q2_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_q2_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yeom\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yeom\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1199\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yeom\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yeom\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yeom\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    220\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer() \n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=31) \n",
    "train_q1_data = vectorizer.fit_transform(train_q1_input).toarray()\n",
    "train_q2_data = vectorizer.fit_transform(train_q2_input).toarray()\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_data\n",
    "# print(train_q1_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-regression",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_q1_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-helicopter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_q2_data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 질문을 하나의 쌍으로\n",
    "train_input = np.stack((train_q1_data, train_q2_data), axis=1)\n",
    "# print(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 훈련 데이터와 평가 데이터로 구분\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_input, eval_input, train_label, eval_label = train_test_split(train_input, train_labels, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost 모델 형식에 맞게 입력갑 가공\n",
    "import xgboost as xgb\n",
    "\n",
    "train_data = xgb.DMatrix(train_input.sum(axis=1), label=train_label)    #학습 데이터 읽어 오기\n",
    "eval_data = xgb.DMatrix(eval_input.sum(axis=1), label=eval_label)       #평가 데이터 읽어 오기\n",
    "\n",
    "data_list = [(train_data, 'train'), (eval_data, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-psychology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 목적함수, 평가지표 설정\n",
    "params = {}\n",
    "params['obejective'] = 'binary:logistic'    # 목적함수 : 이진 로지스틱 함수\n",
    "params['eval_metric'] = 'rmse'  # 평가지표 : root mean square error\n",
    "\n",
    "bst = xgb.train(params, train_data, num_boost_round=1000, evals = data_list, early_stopping_rounds=10)\n",
    "# num_boost_round(=epochs)= 1000, early stopping 10에서 조기 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리한 데이터 가져오기\n",
    "# TEST_Q1_DATA_FILE = 'test_q1.npy'\n",
    "# TEST_Q2_DATA_FILE = 'test_q2.npy'\n",
    "# TEST_ID_DATA_FILE = 'test_id.npy'\n",
    "\n",
    "# test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'))\n",
    "# test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'))\n",
    "# test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, 'rb'))\n",
    "\n",
    "TEST_CLEAN_Q1_DATA = 'test_clean_q1.csv'\n",
    "TEST_CLEAN_Q2_DATA = 'test_clean_q2.csv'\n",
    "\n",
    "test_q1_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_Q1_DATA)\n",
    "test_q2_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_Q2_DATA)\n",
    "\n",
    "test_q1_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1_input = list(test_q1_data['questions1'])\n",
    "test_q2_input = list( test_q2_data['questions2'])\n",
    "test_id_input = list(test_q2_data['test_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_df = pd.DataFrame({'questions1': test_q1_input, 'questions2': test_q2_input, 'id': test_id_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Nan 있는지 확인\n",
    "# df = test_data_df\n",
    "# df_check = test_data_df.isnull()\n",
    "# print(df_check)\n",
    "\n",
    "# check_for_nan = df.isnull().values.any()\n",
    "# print(check_for_nan) #Nan값이 있다면 True출력\n",
    "\n",
    "# total_nan_values = df.isnull().sum().sum()\n",
    "# print(total_nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dop_row = df.dropna(axis=0)  # Nan 값이 있는 행 제거\n",
    "\n",
    "# total_nan_values = df_dop_row.isnull().sum().sum()\n",
    "# print(total_nan_values) #Nan 값이 몇개 있는지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_q1_input = list(df_dop_row['questions1'])\n",
    "# test_q2_input = list(df_dop_row['questions2'])\n",
    "# test_id_input = list(df_dop_row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer() \n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=31) \n",
    "test_q1_data = vectorizer.fit_transform(test_q1_input).toarray()\n",
    "test_q2_data = vectorizer.fit_transform(test_q2_input).toarray()\n",
    "test_id_data = np.array(test_id_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict 함수로 예측\n",
    "test_input = np.stack((test_q1_data, test_q2_data), axis=1)\n",
    "test_data = xgb.DMatrix(test_input.sum(axis=1))\n",
    "test_predict = bst.predict(test_data)\n",
    "\n",
    "# 결괏값 파일로 저장\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "output = pd.DataFrame({'test_id': test_id_data, 'is_duplicate': test_predict})\n",
    "output.to_csv(DATA_OUT_PATH + 'simple_xgb_TF-IDF.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
